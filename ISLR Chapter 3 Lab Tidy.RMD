---
title: "ISLR Chapter 3 Labs"
author: "M. E. Waggoner"
date: "December 30, 2018"
output: 
  html_document:
    toc: yes
    pandoc_args: [
      "--number-sections",
      "--number-offset=2"
    ]
---


# ISLR labs

The labs in the [ISLR textbook](http://www-bcf.usc.edu/~gareth/ISL/) must be used in conjunction with the narrative in order to connect code with theory.  The code here will show how to use `R` to make the calculations and plots, but the interpretation and use of this information is explained in the text.  References to sections of the text are made wherever possible.  The section numbers in this document do not allign with the section numbers in ISLR.

## Advise about model analysis

Don't cherry-pick your statistical analyses.

## Libraries

In this document we will use the `require()` function (instead of the `library()` function) to load *libraries* for use in the `R` code below.  In this chunk of code, called `setup`, we load the packages (`tidyverse`, `knitr`, `broom`, `gridExtra`) for the functions we will use and the packages `ISLR` and `MASS` for the data sets.

Note that the `setup` chunk uses parameters to suppress the code, messages, and warnings.  This is so that these non-people readable items are not in the output.  The parameters (i.e., options) are only for this chunk as they are within the chunk's header.

The option `echo = TRUE` is set for all chunks in the RMarkdown file.  If we want to suppress the code from being echoed for any chunk, we would need to put the parameter `echo = FALSE` in that chunk.

We are including the code in the output of this RMarkdown file, because the file is being used for educational purposes.  If we were creating a report for a different audience, we might suppress all the code.

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)    # for ggplot
require(knitr)        # for kable
require(broom)        # for tidy, glance, and augment
require(gridExtra)    # for grid.arrange
require(ggfortify)    # for autoplot
require(car)          # for vif
require(ISLR)         # for Carseats data set
require(MASS)         # for Boston data set
opts_chunk$set(echo = TRUE)
```

If you get an error upon running this chunk, it is probably that you haven't installed some of the packages.  Use RStudio's **install** option on the packages tab.  Each package only has to be installed once in each project, but the packages have to be reloaded (i.e., `require`d) for each session.

[More information on packages](https://www.datacamp.com/community/tutorials/r-packages-guide)

[More information on the `require()` and `library()` functions](https://stackoverflow.com/questions/5595512/what-is-the-difference-between-require-and-library)


## Simple Linear Regression = one predictor

### The `Boston` data


The `MASS` package contains the `Boston` data set, which records `medv` (median house value) for 506 neighborhoods around Boston.  We seek to predict `medv` using predictors such as 

* `rm` = average number of rooms per house

* `age` = average age of house

* `lstat` = percent of households with low socioeconomic status

Note that the variables are *per neighborhood* and not per house.

Let's take a quick look at the data.  There is some evidence for non-linearity in the relationship between `lstat` and `medv`.  We will explore this issue later in this lab.

(Note that `str(Boston)` can also be viewed in RStudio by clicking the blue arrow in front of `Boston` in the **Environment** tab.)

```{r}

str(Boston)
Boston <- as_tibble(Boston)    # create a tibble from data frame Boston
str(Boston)


ggplot(Boston, aes(x=lstat, y=medv)) +
  geom_point() +
  labs(title = "Median house value against percent of low socioeconomic households",
       subtitle = "Boston data set from the MASS package")



```


###  Formulae in `R`

We can fit linear models using the `lm()` function. For example, we might want to fit a model with `medv` as the response and `lstat` as the predictor, i.e.,

$$medv = f(lstat).$$
To do this we need to know how to define *mathematical functions* in `R`.  The name for a mathematical function $y = f(x)$ in `R` is *formula*, and the notation is 
$$y \sim x.$$
We will talk about how to create more complicated formulae later, but for now we are only interested in linear formulae, and $y \sim x$ is interpreted by `R` as $$y = \beta_0 + \beta_1 x.$$  Thus, to create a linear model of `medv` as a function of `lstat`, we need the `R` formula $$medv \sim lstat.$$

More information on R formulae can be found at [Dr. Waggoner's Math 308 GitHub repository](https://github.com/murphywaggoner/Math-308-Adv-Data-Modeling/blob/master/ISLR%20R%20Formulae.Rmd).

### Generating the linear model and associated statistics

Create the model object with a call to `lm()` and specifying the data.  Recall that `lm()` does not play well with pipes.  Look at the summary data.

```{r}
model_SL <- lm(medv~lstat, data=Boston)

summary(model_SL)
```

Recall that we can extract and save the information about the model using the `broom` functions `tidy()` and `glance()`, and then make them pretty with `kable()`.  

These are the **parameter statistics**, their symbols and calculations in ISLR.

* coefficient (or intercept) = $\hat\beta_i$ = equation (3.4)

* std.error = $SE(\hat\beta_i)$ = equations (3.9) and (3.10)

* statistic = $t$ = equation (3.14)

* p.value = $P(T > |t|)$ = `1 - pt(abs(t), df = n - 2)`

Here are selected **model statistics**, their symbols and calculations (if possible) in ISLR.

* r.squared = $R^2$ = equation (3.17)

* sigma = $RSE = \sigma_{\epsilon}$ = equation (3.15)

* statistic = $F$ = equation (3.23)

* p.value = $P(f > F)$ = `1 - pf(F, df1 = p, df2 = n - 2)`

* adj.r.squared, AIC and BIC are discussed in ISLR Chapter 6 

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.  The calculation of these intervals is equation (3.11) in Section 3.1.2, and the interpretation is illustrated in Section 3.4, part 4.



```{r}
param_stats <- tidy(model_SL) 
kable(param_stats,
      caption = "Statistics related to each parameter")

1-pt(abs(-24.5), df = nrow(Boston) - 1)

model_stats <- glance(model_SL)
kable(model_stats,
      caption = "Statistics related to the entire model")


confint(model_SL)  # Confidence intervals for the coefficients

```

### Plotting the data and the fit

You have probably seen plots using `geom_smooth()` with `method = "lm"` to create plots like the following with "confidence" intervals around the predicted line.  

It is important to include the *method* and *formula*, so that geom_smooth will be using the methods you expect.

What is the relationship between this plot and the `lm` model we created above?

```{r}
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm",
              formula = y ~ x) +
  labs(title = "Linear regression line of medv v. lstat with confidence interval",
       subtitle = "generated by ggplot() and geom_smooth()")
```

To create the same plot "by hand", we need the fitted values and the standard error (SE) for each one.  The `augment()` function from the `broom` package will create a tibble with the values used in the linear regression, fitted values, residuals, standard errors, etc. 

Here we create a tibble using the original values for `lstat` and give it a look-see.  We can plot the data and  a line through the fitted values, but we won't have the confidence intervals we saw in `geom_smooth()`.

```{r}

model_SL_data <- augment(model_SL)

kable(head(model_SL_data))

model_SL_data %>% 
  ggplot(aes(lstat, medv)) + 
  geom_point() +
  geom_line(aes( y = .fitted),
            color = "red") +
  labs(title = "Linear regression line of medv v. lstat",
       subtitle = "generated by lm() and augment()")
```


To add the confidence intervals, we need to calculate a confidence interval of the form $$(\hat y - r SE,\ \  \hat y + r SE )$$ for for each $\hat y$, where $r SE$ is the radius of the interval.  The function `geom_smooth()` uses a confidence level of $c = 95%$ by default, and the distribution of the $\hat y$ values is approximated with a student's $t$-distribution with $n - 2$ degrees of freedom.  Thus, the value of $r$ gives $$P(t \leq r) = 97.5\%,$$ and can be calculated with the `qt()` probability function.
```{r}

r <- qt(0.975, nrow(Boston))

myPlot <- model_SL_data %>% 
  ggplot(aes(lstat, medv)) + 
  geom_point() +
  geom_line(aes( y = .fitted),
            color = "red") +
  geom_ribbon(aes( ymax = .fitted + r* .se.fit,
                   ymin = .fitted - r* .se.fit),
              fill = "gray",
              alpha = 0.5)+
  labs(title = "Linear regression line of medv v. lstat with confidence interval",
       subtitle = "generated by lm() and augment()")

myPlot

```

The `augment()` function can also be used to produce fitted values and standard errors (SE) for new values of the input variable. First, we'll make a data frame with some new values for `lstat` and then call `augment()`.  We then plot these on the graph from above.

```{r}
new_values <- tibble(lstat=c(5,10,15))

new_fit <- augment(model_SL, newdata = new_values)

myPlot +
  geom_point(aes(x = lstat, y = .fitted),
             data = new_fit,
             size = 3, 
             color = "green") +
  labs(caption = "Three fitted points shown in green")
```

### Residual analysis plots

Next we examine some diagnostic plots, several of which were discussed in Section 3.3.3.  Four diagnostic plots are automatically generated by the `lm()` function, and `autoplot()` does a nice job of making a grid of these plots.

```{r}
autoplot(model_SL)
```

Alternatively, we can compute the residuals from a linear regression fit using the `residuals()` function. The function `rstudent()` will return the studentized residuals, and we can use this function to plot the residuals against the fitted values. To draw these plots side by side, we can use the `grid.arrange()` function from the `gridExtra` library. We'll also use the `labs()` function to add labels to our plots.

```{r}


plot1 <- ggplot() +
  geom_point(aes(predict(model_SL), residuals(model_SL))) +
  labs(x="Predicted Value", y="Residual") + 
  geom_hline(aes(yintercept = 0), alpha = 0.3)

plot2 <- ggplot() +
  geom_point(aes(predict(model_SL), rstudent(model_SL))) +
  labs(x="Predicted Value", y="Studentized Residual") + 
  geom_hline(aes(yintercept = 0), alpha = 0.3)

grid.arrange(plot1, plot2, ncol=2)
```

On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the `hatvalues()` function.

```{r}

hats <- tibble(hat = hatvalues(model_SL), index = c(1:nrow(Boston)))
hats %>% 
  ggplot(aes(x = index, y = hat)) +
  geom_point()
```

The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.

```{r}
which.max(hats$hat)
```

Analysis of these residual and leverage plots is in ISLR Section 3.3.3.

* Part 1: Non-linearity of the data uses residual and QQ-plots, plotting $(x, y-\hat y)$ for simple regression and $(\hat y, y - \hat y)$ for multiple regression in Figure 3.9.

* Part 2: Correlation of error terms uses simulated time series plots, plotting $(index, y-\hat y)$ in Figure 3.10.

* Part 3:  Non-constant variance of error terms uses residual plots, plotting plotting $(x, y-\hat y)$ or $(\hat y, y - \hat y)$, as appropriate, in Figure 3.11.

* Part 4: Outliers uses studentized residuals, plotting $(\hat y, (y - \hat y)/RSE)$ in Figure 3.12.

* Part 5: High leverage points uses plots of leverage, plotting $(h, (y-\hat y)/RSE)$ in Figure 3.13.  The calculation of $h$ is in equation (3.37).

## Multiple Linear Regression

### Various formulations of multiple regression with statistics

#### Linear with two inputs and related statistics

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y ∼ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. In this first multiple regression, we are looking for a linear function $medv = f(lstat, age)$.

The `summary()` function now outputs the regression coefficients for all the predictors.  But we can use `tidy()`, `glance()`, and `confint()` as we did for simple regression.

```{r}
model_ML1 = lm(medv ~ lstat + age, data=Boston)
summary(model_ML1)

param_stats <- tidy(model_ML1) 
kable(param_stats,
      caption = "Statistics related to each parameter")

model_stats <- glance(model_ML1)
kable(model_stats,
      caption = "Statistics related to the entire model")


confint(model_ML1)  # Confidence intervals for the coefficients
```

#### Linear with all inputs

The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:

```{r}
model_ML_all = lm(medv ~ ., data=Boston)

formula(model_ML_all)
```

#### Linear on all but one input

What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, `age` has a high $p$-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except `age`.

```{r}
model_ML_no_age = lm(medv ~ . - age, data=Boston)
formula(model_ML_no_age)
tidy(model_ML_no_age)



```


Alternatively, we can use the `update()` function to return an updated version of our previous `model_ML`.

```{r}
model_ML_no_age = update(model_ML_all, ~ . - age)
formula(model_ML_no_age)
```


#### Non-linear with interaction terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells R to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term $lstat\cdot age$ as predictors; it is a shorthand for `lstat+age+lstat:age`.

ISLR explains how to interpret coefficients of models with interaction terms in Section 3.3.2, along with their advantages and pitfalls.

```{r}
tidy(lm(medv ~ lstat:age, data = Boston))
tidy(lm(medv ~ lstat * age, data = Boston))
```


#### Non-linear with powers

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor `X`, we can create a predictor `X2` using `I(X^2)`. The function `I()` is needed since the ^ has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise `X` to the power 2. We now perform a regression of `medv` onto `lstat` and `lstat2^2`.

```{r}
model_ML2 = lm(medv ~ lstat + I(lstat^2), data=Boston)
tidy(model_ML2)
```


Alternately, we can use the `poly()` formula with `raw = TRUE`.  You can read an [explanation of how `poly()` creates orthogonal polynomials](https://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do) when `raw = FALSE`, and why that is a good thing.

```{r}
model_ML2 = lm(medv ~ poly(lstat, 2, raw = TRUE), data=Boston)
tidy(model_ML2)


```


In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a fifth-order polynomial fit

```{r}
model_ML5 = lm(medv ~ poly(lstat, 5, raw=TRUE), data=Boston)
tidy(model_ML5)
```



#### Non-linear with other functions

Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.

```{r}
tidy(lm(medv ~ log(rm), data=Boston))
```


#### More information

More information on R formulae can be found at [Dr. Waggoner's Math 308 GitHub repository](https://github.com/murphywaggoner/Math-308-Adv-Data-Modeling/blob/master/ISLR%20R%20Formulae.Rmd).



### Variance inflation factors

The `vif()` function, part of the `car` package, can be used to compute variance inflation factors. Most VIFs are low to moderate for this data. 

The VIF statistic is used to determine *collinearity* and is discussed in ISLR Section 3.3.3 part 6.

```{r}
kable(vif(model_ML_all), 
      col.names = c("VIF"))
```



### Comparing models with ANOVA

#### Comparing two models

Recall the first linear model using `lstat` and the quadratic model on the same variable.

```{r}
model_SL <- lm(medv ~ lstat, data=Boston)
model_ML2 <- lm(medv ~ lstat + I(lstat^2), data=Boston)
tidy(model_SL)
tidy(model_ML2)
model_SL_data <- augment(model_SL)
model_ML2_data <- augment(model_ML2)

r <- qt(0.975, nrow(Boston))

SLPlot <- model_SL_data %>% 
  ggplot(aes(lstat, medv)) + 
  geom_point() +
  geom_line(aes( y = .fitted),
            color = "red") +
  geom_ribbon(aes( ymax = .fitted + r* .se.fit,
                   ymin = .fitted - r* .se.fit),
              fill = "gray",
              alpha = 0.5)+
  labs(title = "Linear regression line of medv v. lstat with confidence interval")

ML2Plot <- model_ML2_data %>% 
  ggplot(aes(lstat, medv)) + 
  geom_point() +
  geom_line(aes( y = .fitted),
            color = "red") +
  geom_ribbon(aes( ymax = .fitted + r* .se.fit,
                   ymin = .fitted - r* .se.fit),
              fill = "gray",
              alpha = 0.5)+
  labs(title = "Quadratic regression line of medv v. lstat, lstat^2 with confidence interval")

grid.arrange(SLPlot, ML2Plot)
```


The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r}
anova(model_SL, model_ML2)
```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat2`. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. 

The $F$-statistic for the ANOVA is 135, and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. 

If we compare the residual plots of the quadratic model to those of the linear model, we see an improvement in the fit, and that  when the `lstat^2` term is included in the model, there is little discernible pattern in the residuals.

```{r}
autoplot(model_SL)
autoplot(model_ML2)
```

#### Compraring three models

ANOVA is a two-way comparison.  To compare three models, we do this pairwise.  

```{r}
anova(model_SL, model_ML5)
anova(model_ML2, model_ML5)
```

This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values in a regression fit.  More about model selection is in later chapters of ISLR.

#### More information

[More information on ANOVA as a statistical test.](https://www.youtube.com/watch?v=ITf4vHhyGpc)

[More information on ANOVA as a way to compare models.](https://bookdown.org/ndphillips/YaRrr/comparing-regression-models-with-anova.html)


## Qualitative Predictors

### The `Carseats` data

We will now examine the `Carseats` data, which is part of the `ISLR` library. We will attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors.

The `Carseats` data includes qualitative predictors such as `Shelveloc`, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor `Shelveloc` takes on three possible values, `Bad`, `Medium`, and `Good`, and is stored as a factor in R.

```{r}

str(Carseats)

```

### Dummy variables

Below we fit a multiple regression model that includes some interaction terms. For qualitative variable such as `Shelveloc`,  the `lm()` function generates dummy variables automatically.  ISLR explains this in Section 3.3.1.

```{r}
Model_CS <- lm(Sales ~ . +Income:Advertising + Price:Age, 
               data = Carseats)
formula(Model_CS)
tidy(Model_CS)
```

The `contrasts()` function returns the coding that R uses for the dummy variables. Use `?contrasts` to learn about other contrasts, and how to set them.

```{r}
contrasts(Carseats$ShelveLoc)
```

## Sources and Attributions

This lab on Linear Regression in R comes from pp. 109-119 of "Introduction to Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was re-implemented in Fall 2016 in `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College.
M. E. Waggoner at Simpson college made additional changes to implement `broom` and add content and was not as faithful to the original ISLR labs as the Smith College faculty were.  However, this document tries to make a stronger connection between the `R` code of the labs and the theory by providing references to the text and other resources.  
