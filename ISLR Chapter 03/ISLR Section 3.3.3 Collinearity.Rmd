---
title: "Collinearity"
author: "M. E. Waggoner"
date: "October 11, 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

numpts = 200

x1<-3*runif(numpts)+5
x2<-10*rnorm(numpts)+30
x3<-jitter(-4*x1+7*x2+runif(numpts))
x4<-200-rf(numpts, df1=10, df2=15)
y<-jitter(100*x1+200*x2-55*x3+90)
iscol <- data.frame(x1=x1,
                     x2=x2,
                     x3=x3,
                     x4=x4,
                     y=y
                     )

x1<-3*runif(numpts)+5
x2<-10*rnorm(numpts)+30
x3<-jitter(5*x1**2+40*sin(x2))
x4<-200-rf(numpts, df1=10, df2=15)
y<-jitter(100*x1+200*x2-55*x3+90)
noncol <- data.frame(x1=x1,
                     x2=x2,
                     x3=x3,
                     x4=x4,
                     y=y
                     )

library(car)

```

## What is collinearity?

- *Collinearity* or *multicollinearity* occurs when one predictor is, or nearly is, a linear combination of the other predictors.

- The predictors have a linear relationship of the form $$x_j = a_1 x_1 + a_2 x_2 + \cdots$$

- If there is colinearity between predictors, the coefficients of the model have no meaning.

- Material in this presentation is taken from Frees **Regression Modeling with Actuarial and Financial Applications**

## An accidental example

$$a_1 \begin{bmatrix}3\\2\end{bmatrix}+ 
a_2 \begin{bmatrix}2\\-3\end{bmatrix}+
a_3 \begin{bmatrix}5\\-1\end{bmatrix}+
a_4 \begin{bmatrix}-4\\6\end{bmatrix}
= \begin{bmatrix}0\\0\end{bmatrix}$$

- In linear algebra  I set up this vector equation knowing that the coefficients that  would satisfy this equation were $(a_1,a_2,a_3,a_4) = (1,1,-1,0).$



- However, a student gave the answer $(a_1,a_2,a_3,a_4) = (0,1,0,-2).$

- How could we both be right?  And how does this relate to linear regression?

## Relationship to Linear Regression

- To find the coefficients of a linear regression, we are using a matrix of the form

$$ \begin{bmatrix}Variables&x_1&x_2&x_3&x_4&y\\
Row1&3& 2& 5& -4& 0\\ 
Row2&1& -3& -2& 12&0\end{bmatrix}$$
and we are looking for coefficients $a_i$ such that
$y = a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4$ as closely as possible.


- If there is collinearity ($x_3 = x_1 + x_2$ and $x_4 = -2x_2$), there are infinitely many solutions to the system.  

- Since the columns of the coefficient matrix are linearly dependent, the equation $A\mathbf{x}=\mathbf{y}$ does not have just one solution and the student and I got different ones.

## Why does this effect interpretatability?

- Two possible solutions are 
$(a_1,a_2,a_3,a_4) = (0,2,0,-1)$ and
$(a_1,a_2,a_3,a_4) = (1,3,-1,-1)$

- Thus, if the matrix above is data for a linear regression, then both of the equations 
$$y = 0x_1+2x_2+0x_3-1x_3$$ 
and
$$y = 1x_1+3x_2-1x_3-1x_3$$
could be linear regression lines for the same data.  

- Therefore, the coefficients of a linear regression with collinearity cannot be interpreted as $\displaystyle a_i = \frac{\partial y}{\partial x_i}$

## Variance Inflation Factors (VIFs)

- Process for calculating VIF

    - For each $j$, run a regression of $x_j \sim x_1 + x_2 + \cdots$

    - Calculate $R_j^2$ for each of these regressions recalling that $0 \leq R_j^2 \leq 1$ 

    - Then $VIF_j = \frac{1}{1-R_j^2}$

- Large $R_j^2$ results in large $VIF_j$ 

- $VIF_j = \frac{1}{1-R_j^2} > 10 \Rightarrow R_j^2 > 90\%$

## Collinear Data

- In this example, we see that the VIF scores are high.  This indicates collinearity

```{r, echo = TRUE}
iscol.fit <-lm(y~x1+x2+x3+x4,data=iscol)
vif(iscol.fit)
```

- On the next slide we see that the plot of the data confirms that there is collinearity between the $x_i$.

## Collinear Data Plot

```{r}
plot(iscol)
```

## Non-collinear Data

- In this example, we see that the VIF scores close to 1.  This indicates no collinearity between the $x_i$

```{r, echo = TRUE}
noncol.fit <-lm(y~x1+x2+x3+x4,data=noncol)
vif(noncol.fit)
```

- On the next slide we see that the plot of the data confirms that two variables are collinear.

## Non-collinear Data Plot

```{r}
plot(noncol)
```


## Collinearity Facts

- Fitting data with collinearity of the $x_i$ can still result in good fits and still allows for predictions of new observations

-  Estimates of error variances and tests of model adquacy are still reliable

- In cases of serious collinearity, standind errors of individual regression coefficients are greater than were collinearity does not exist

- It is difficult to detect the importance of a variable because of the high standard errors 

- The coefficients of collinear predictors will not be meaningful

## 

