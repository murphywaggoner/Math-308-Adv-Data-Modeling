---
title: "ISLR Chap 3 Notes"
author: "M. E. Waggoner"
date: "February 6, 2019"
output: 
  pdf_document:
    toc: yes
    toc_depth: 5
    pandoc_args: [
      "--number-sections"
    ]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      fig.width = 6 ,
                      fig.height = 3)
require(tidyverse)    # various including as_factor()
require(texreg)       # screenreg()
require(ggfortify)    # autoplot()
require(knitr)        # kable()
require(broom)        # tidy()
require(car)          # vif()

options(digits = 4)



# A function to extract and print the F-statistic
tidyF <- function(lmobj) {
  stats <- summary(lmobj)$fstatistic
  prob <- 1- pf(stats[["value"]], 
                df1 = stats[["numdf"]], 
                df2 = stats[["dendf"]])
  tibble(`F-statistic` = c(stats[["value"]]),
         df1 = c(stats[["numdf"]]), 
         df2 = c(stats[["dendf"]]),
         `p-value` = c(prob)
         )
  }

```


$\setcounter{section}{2}$

#  Linear Regression

$\setcounter{subsection}{2}$
$\setcounter{equation}{25}$

Note:  This section is incomplete.  An updated version will be posted on Scholar.  The section numbers of this document match up with ISLR.  The equation numbers match up with the section, but you need to append "3." to the number.

## Other Considerations in the Regression Model

### Qualitative Predictors

A factor variable with $n$ levels will generate $n-1$ dummy variables to store the information.

#### Predictors with Only Two Levels

If `gender` is a factor variable with two *levels*, R will create a *dummy variable* that takes on two values

\begin{align}
x_i = \left\{ \begin{array}{ll} 
                1 & \text{if }i\text{th person is female} \\
                0 & \text{if }i\text{th person is not female} \\
                \end{array} \right. .
\end{align}

Since $x_i$ only takes on two values, 0 or 1, the model becomes 

\begin{align*}
y_i = \beta_1 x_i + \beta_0 = \left\{ \begin{array}{ll} 
                \beta_1(1) + \beta_0 & \text{if }i\text{th person is female} \\
                \beta_1(0) + \beta_0 & \text{if }i\text{th person is not female} \\
                \end{array} \right. 
\end{align*}

or 

\begin{align}
y_i = \beta_1 x_i + \beta_0 = \left\{ \begin{array}{ll} 
                \beta_1 + \beta_0 & \text{if }i\text{th person is female} \\
                \beta_0 & \text{if }i\text{th person is not female} \\
                \end{array} \right. .
\end{align}

Thus, 

* $\beta_0$ is the contribution to the value of $y$ among males,

* $\beta_1$ + $\beta_0$ is the  value of $y$ among females, and

* $\beta_1$ is the difference in the contribution to the value of $y$ between females and males.

#### Qualitative Predictors with More than Two Levels

If `gender` were a factor variable with three *levels* (female, male, undisclosed), R will create two *dummy variables*, each that take on two values

\begin{align}
x_{i1} = \left\{ \begin{array}{ll} 
                1 & \text{if }i\text{th person is female} \\
                0 & \text{if }i\text{th person is not female} \\
                \end{array} \right.
\end{align}

and

\begin{align}
x_{i2} = \left\{ \begin{array}{ll} 
                1 & \text{if }i\text{th person is male} \\
                0 & \text{if }i\text{th person is not male} \\
                \end{array} \right. .
\end{align}

The model becomes 

\begin{align*}
y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_0 = \left\{ \begin{array}{ll} 
                \beta_1 + \beta_0 & \text{if }i\text{th person is female} \\
                \beta_2 + \beta_0 & \text{if }i\text{th person is male} \\
                \beta_0 & \text{if }i\text{th person's gender is undisclosed} \\
                \end{array} \right. .
\end{align*}

Note that $x_{i1}$ and $x_{i2}$ will never both be 1 for the same observation.  We can interpret the coefficients in the following way: 

* $\beta_0$ is the contribution to the value to $y$ from someone from having undisclosed gender as opposed to disclosing gender,

* $\beta_1 + \beta_0$ is the contribution to the value of $y$ from being female as opposed to not being female, and

* $\beta_2 +\beta_0$ is the contribution to the value of $y$ from being male as opposed to not being male.



##### Example: Using factors and interpreting dummy variables

Here we will read in a dataset about solar flares, which was chosen because it has ten categorical predictors.  We will make some changes to those variables so that the output is more people readable, investigate the structure of factor variables, and see them used in a regression.

Information about this dataset can be found at the [UCI database repository](https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.names).  The classification codes can be found at the [Solar Influences Data Analysis Center](http://sidc.oma.be/educational/classification.php).

This dataset has a first line that is not an observation, and so we skip it.  There are no variable names in the webpage, and so we add the names by hand.  R will not parse the factors that have numeric levels as factors, and so we tell `read_delim()` to parse the first 10 columns as factor variables and the last three as numeric with the option `col_types = "fffffffnnn".`

```{r echo = TRUE}
flare <- 
  read_delim("https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.data1", 
             delim = " ", 
             skip = 1, 
             col_names = c("ZClass", "Size", "Distr",
                          "Activity", "Evolution", "Previous",
                          "Complex", "New", "Area",
                          "Largest", "C_Class", "M_Class",
                          "X_Class"),
             col_types = "ffffffffffnnn"
             )

kable(head(flare), align = "ccccccccccccc")


str(flare$Activity)
str(flare$Evolution)
str(flare$Distr)


```

However, from the documentation we know that the numbers in variables like `Activity` represent something more meaningful:  1 => reduced, 2 => unchanged.  Factor variables can store numeric values and display more meaningful labels 

```{r echo = TRUE}
flare$Distr <- dplyr::recode(flare$Distr, 
                      X = "Undefined", 
                      O = "Open",
                      I = "Intermediate", 
                      C = "Compact")

flare$Activity <- dplyr::recode(flare$Activity,
                         "1" = "Reduced", 
                         "2" = "Unchanged")
flare$Evolution <- dplyr::recode(flare$Evolution, 
                   "1" = "Decay", 
                   "2" = "None", 
                   "3" = "Growth")
flare$Previous <- dplyr::recode(flare$Previous,
                         "1" = "LTM1",
                         "2" = "M1",
                         "3" = "GTM1",
                         .ordered = "TRUE")
flare$Complex <- dplyr::recode(flare$Complex,
                        "1" = "Yes", 
                        "2" = "No")
flare$New  <- dplyr::recode(flare$New,
                        "1" = "Yes", 
                        "2" = "No")
flare$Area  <- dplyr::recode(flare$Area,
                      "1" = "Small", 
                      "2" = "Large",
                      .ordered = "TRUE")
           

flare$Largest <- dplyr::recode(flare$Largest,
                        "1" = "LT5", 
                        "2" = "GT5",
                        .ordered = "TRUE")

kable(head(flare, align = "ccccccccccccc"))

str(flare$Activity)
str(flare$Evolution)
str(flare$Distr)

```

A factor variable with $n$ levels will need $n-1$ dummy variables, and `lm()` as well as other R functions will create the dummy variables for us.

The `contrasts()` function illustrates what `lm()` will do.  All levels are along the left side, the dummy variables along the top, and the values of each dummy variable that result in each level.  The variable missing from the  dummy variables is the *baseline*.  Baselines are arbitrary and may vary from function to function or one installation of R to another.

```{r}
kable(contrasts(flare$Activity), caption = "Activity")
kable(contrasts(flare$Evolution), caption = "Evolution")
kable(contrasts(flare$Distr), caption = "Distribution")


```

How can we read the results of using categorical variable  as predictors in a linear model?  Consider this output.

```{r}
lmSolar <- lm(C_Class ~ Activity + Evolution + Distr, 
              data = flare)

kable(tidy(lmSolar))

kable(tidyF(lmSolar))

```


The *baseline* of the factor variable is arbitrary, and the coefficients of each dummy variable depend on the baseline value.   In addition, we should consider the dependent variable to be a function of the *factor variable*, not the *dummy variables*.  Thus, the $p$-values associated with each coefficient of the dummy variables is not a useful as it was for numeric variables.

The $p$-value for the $F$-statistic (0.0013 < 0.05) indicates that we would reject the null hypothesis that $\beta_{Activity} = \beta_{Evolution}= \beta_{Distr} = 0$, and accept that there is a relationship between `C_Class` and these three predictors.

Consider the coefficients of `DistrIntermediate`, `DistrUndefined` and `DistrCompact` and the fourth factor, "Open", in `Distribution`.   These interpretations are different than the coefficients of numeric variables.  We interpret these based on the dummy variable being yes or no, and not as a slope.  

Assuming that `Activity` and `Evolution` are fixed:

*  The small positive value $\beta_{DistrIntermediate} = 0.0051$ indicates that an active region in the sun with an intermediate distribution of sun spots contributes 0.0051 more to the number of C-class flares than any region without an intermediate distribution of sun spots.

* The negative value $\beta_{DistrUndefined} = -0.0740$ indicates that an active region in the sun with an undefined distribution of sun spots contributes 0.0740 *less* to the number of C-class flares than any region without an undefined distribution of sun spots.

* The positive value $\beta_{DistrComplex} = 0.2819$ indicates that an active region in the sun with an complex distribution of sun spots contributes 0.2819 more to the number of C-class flares than any region without an complex distribution of sun spots.



### Extensions of the Linear Model

#### Removing the Additive Assumption

The additive assumption is that $$\frac{\partial Y}{\partial X_i}$$ is independent of all $X_j$ where $j \neq i$.  For a linear equation $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots,$$ the partials $$\frac{\partial Y}{\partial X_i} = \beta_i$$ are constant, and hence, satisfy the additive assumption.

On the other hand, if the equation is $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2,$$ then $$\frac{\partial Y}{\partial X_1} = \beta_1 + \beta_3 X_2,$$ and is dependent on $X_2$.  

Some models need interaction terms because the change in the outcome is sometimes based on interaction.  Consider a predator-prey model where change in population depends on the interactions of predator and prey.

#### Non-linear Relationships

### Potential Problems

#### Non-linearity of the Data

If the discernible patterns in the data are not captured by the model, then the residuals will exhibit a non-random pattern.  If the residuals exhibit one of these behaviors, we should work toward a different model and the behaviors of the residuals should help us know what that model should do.

The diagnostic tools for observing non-linearity of the data are residual plots; that is, plots that show or analyze points of the form

\begin{align*}
(x, y- \hat{y})& \text{for simple linear regression and} \\
(\hat{y}, y - \hat{y})& \text{for multiple linear regression.}
\end{align*}

* Simple remedy: transform $X$ using powers, logs, roots, exponentials

* More advance remedies:  See later in the book

##### Example of residual analysis for non-linearity



#### Correlation of Error Terms

#### Non-constant Variance of Error Terms

#### Outliers = unusual $y$ values

We will not focus on outliers in this class, but a great project would be to find and understand a case study of data with outliers.

#### High Leverage Points = unusual $x$ values

We will not focus on high leverage points in this class, but a great project would be to find and understand a case study of data with high leverage points.

#### Collinearity


```{r include=FALSE}
# Creating data from the following examples.

numpts = 200

x1<-3*runif(numpts)+5
x2<-10*rnorm(numpts)+30
x3<-jitter(-4*x1+7*x2+runif(numpts))
x4<-200-rf(numpts, df1=10, df2=15)
y<-jitter(100*x1+200*x2-55*x3+90)
iscol <- tibble(x1=x1,       # a data set with collinearity
                     x2=x2,
                     x3=x3,
                     x4=x4,
                     y=y
                     )

x1<-3*runif(numpts)+5
x2<-10*rnorm(numpts)+30
x3<-jitter(5*x1**2+40*sin(x2))
x4<-200-rf(numpts, df1=10, df2=15)
y<-jitter(100*x1+200*x2-55*x3+90)
noncol <- tibble(x1=x1,      # a data set with little collinearity
                     x2=x2,
                     x3=x3,
                     x4=x4,
                     y=y
                     )



```

* *Collinearity* or *multicollinearity* occurs when one predictor is, or nearly is, a linear combination of the other predictors.  

  * In other words, when the predictor columns of the dataset are linearly dependent, and the predictors have a linear relationship of the form $$x_j = a_1 x_1 + a_2 x_2 + \cdots$$

Material in this section is taken from Frees **Regression Modeling with Actuarial and Financial Applications**

*An accidental example from linear algebra*

$$\beta_1 \begin{bmatrix}3\\2\end{bmatrix}+ 
\beta_2 \begin{bmatrix}2\\-3\end{bmatrix}+
\beta_3 \begin{bmatrix}5\\-1\end{bmatrix}+
\beta_4 \begin{bmatrix}-4\\6\end{bmatrix}
= \begin{bmatrix}0\\0\end{bmatrix}$$

In linear algebra  I set up this vector equation knowing that the coefficients that  would satisfy this equation were $$(\beta_1,\beta_2,\beta_3,\beta_4) = (1,1,-1,0).$$  However, a student gave the answer $$(\beta_1,\beta_2,\beta_3,\beta_4) = (0,1,0,-2).$$  How could we both be right?  And how does this relate to linear regression?

*Linear dependence of predictors = collinearity* To find the coefficients of a linear regression, we are using a matrix of the form $$ \begin{bmatrix}Variables&x_1&x_2&x_3&x_4&y\\
Row1&3& 2& 5& -4& 0\\ 
Row2&1& -3& -2& 12&0\end{bmatrix}$$ and we are looking for coefficients $\beta_i$ such that $y = \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4$ as closely as possible. The columns of data are linearly dependent since $x_3 = x_1 + x_2$ and $x_4 = -2x_2$.  Thus, the columns of the coefficient matrix are linearly dependent, the equation $B\mathbf{x}=\mathbf{y}$ has infinitely many solutions, and the student and I got different ones.

*Why does this effect interpretability of linear regression?*
If the matrix above is data for a linear regression, then both of the equations 
$$y = 0x_1+2x_2+0x_3-1x_3$$ 
and
$$y = 1x_1+3x_2-1x_3-1x_3$$
could be linear regression lines for the same data.  Therefore, the coefficients of a linear regression with collinearity cannot be interpreted as $\displaystyle \beta_i = \frac{\partial y}{\partial x_i}$

**Variance Inflation Factors (VIFs)**


* For each $j$, run a regression of $x_j \sim x_1 + x_2 + \cdots$

* Calculate $R_j^2$ for each of these regressions recalling that $0 \leq R_j^2 \leq 1$ 

$$VIF_j = \frac{1}{1-R_j^2}$$

* Values of $R_j^2$ close to 1 result in large $VIF_j$, and we see that when $VIF_j > 10$ the correlation between $x_j$ and the other predictors is greater than 90%.


*Example:  Collinear Data*

In this example, we see that the VIF scores are high.  This indicates collinearity and the pairs plot of the data confirms that there is collinearity between the $x_i$.

```{r, echo = FALSE}
iscol.fit <-lm(y~x1+x2+x3+x4,data=iscol)
kable(vif(iscol.fit), caption = "Large VIF scores")

plot(iscol[,1:4])# a non-tidyverse way of plotting - don't use
```

*Example:  Non-collinear Data*

In this example, we see that the VIF scores are relatively close to 1.  This indicates no collinearity between the $x_i$.  However we see from the `pairs` plot that the variables with higher VIF scores have more *correlation* even though they are *non-collinear*.

```{r echo = FALSE}
noncol.fit <-lm(y~x1+x2+x3+x4,data=noncol)
kable(vif(noncol.fit), caption = "VIF scores less than 10")

plot(noncol[,1:4]) # a non-tidyverse way of plotting - don't use
```


*Summary:* If there is collinearity in the input variables, *prediction* can still be done.

* ...fitting data can  result in good fits and *predictions* of new observations are valid

*  ...also estimates of error variances and tests of model accuracy are still reliable

However, in cases of serious collinearity, *inference* cannot be done because of the lack of uniqueness of the *coefficients*.

* ...standard errors of individual regression coefficients are greater than if collinearity does not exist

* ...the importance of a variable cannot be detected because of the high standard errors 

* ...the coefficients of collinear predictors are not meaningful



