---
title: "ISLR Chap 3 Notes"
author: "M. E. Waggoner"
date: "February 3, 2019"
output: 
  html_document:
    toc: yes
    pandoc_args: [
      "--number-sections",
      "--number-offset=2"
    ]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The statistics behind most machine learning assumes that the data we have is just one sample of a much larger population.  Thus, when we calculate means and standard deviations, we are calculating them from a sample, i.e., 
$$\displaystyle\bar{x} = \frac{1}{n}\Sigma_{i-1}^n x_i.$$
Sample statistics methods are what are behind the hypothesis testing and confidence intervals we will use.

# Simple Linear Regression

$y$ is aproximately a linear function of $X$, but unknown
\begin{align}
Y \approx \beta_0 + \beta_1 X
\end{align}

Since we don't know the slope and intercept, we find approximations for them using the training data.
\begin{align}
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x
\end{align}

## Estimating the Coefficients

$i$th Residual.  Note that this is the difference between the actual value and the estimated linear relationship, which is not the same as $\epsilon_i$, which is the difference between the actual value and the true line.
\begin{align*}
e_i = y_i - \hat{y}_i 
\end{align*}

Residual sum of squares = what is minimized by least squares.  I  said that MSE was minimized by least squares, and they are since $MSE = RSS/n$. 
\begin{align}
	\mathrm{RSS} =& e_1^2	+ e_2^2 + \cdots + e_n^2 \nonumber \\ 
	=& \left( y_1 - \hat{y}_1\right)^2 + \left( y_2 - \hat{y}_2\right)^2 + \cdots + \left( y_n - \hat{y}_n\right)^2 \\
	=& \Sigma_{i-1}^n \left( y_i - \hat{y}_i\right)^2 \nonumber
\end{align}

Least squares coefficients estimates that minimize RSS where $\bar{x}$ and $\bar{y}$ are 
\begin{align}
\hat{\beta}_1 =& \frac{\Sigma_{i = 1}^{n} \left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}
                    {\Sigma_{i = 1}^{n} \left(x_i - \bar{x}\right)^2} \nonumber \\
\hat{\beta}_0	=& \bar{y} - \hat{\beta}_1 \bar{x},
\end{align}
where $\bar{x}$ and $\bar{y}$ are means of $x$ and $y$, respectively

## Assessing the Accuracy of the Coefficient Estimates

True linear relationship between $X$ and $Y$
\begin{align}
	Y = \beta_0 + \beta_1 X + \epsilon
\end{align}

The standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution or an estimate of that standard deviation.

Standard error of a sample mean where $\sigma^2 = \mathrm{Var}(x)$ for a population $X$
\begin{align}
	\mathrm{Var}\left(\hat{\mu}\right) = \mathrm{SE}\left(\hat{\mu}\right)^2 = \frac{\sigma^2}{n}
\end{align}

Standard error of estimates of linear coefficients where $\sigma^2 = \mathrm{Var}(\epsilon)$
\begin{align}
	\mathrm{SE}\left(\hat{\beta}_0\right)^2 & = 
		\sigma^2\left[ 
			\frac{1}
					 {n} + 
			\frac{\bar{x}^2}
					 {\Sigma_{i - 1}^n\left(x_i - \bar{x}^2\right)} \right] 
			\nonumber \\
	\mathrm{SE}\left(\hat{\beta}_1\right)^2 & = 
		 \frac{\sigma{x}^2}
					{\Sigma_{i - 1}^n \left(x_i - \bar{x}^2\right)}
\end{align}

There is approximately a 95\% chance that the true value of the slope $\beta_1$ lies in the interval
\begin{align}
	\left[ 	\hat{\beta}_1 - 2 \mathrm{SE}\left( \hat{\beta}_1 \right),
					\hat{\beta}_1 + 2 \mathrm{SE}\left( \hat{\beta}_1 \right) 
					\right].
\end{align}

There is approximately a 95% chance that the true value of the intercept $\beta_0$ lies in the interval
\begin{align}
	\left[ 	\hat{\beta}_0 - 2 \mathrm{SE}\left( \hat{\beta}_0 \right),
					\hat{\beta}_0 + 2 \mathrm{SE}\left( \hat{\beta}_0 \right) 
					\right].
\end{align}

A common hypothesis test of linear relationships tests the null hypothesis
\begin{align}
	H_0&: \text{There is no relationship between }X\text{ and }Y\nonumber \\
	H_0&: \beta_1 = 0
\end{align}
versus the alternate hypothesis
\begin{align}
	H_1&: \text{There is a relationship between }X\text{ and }Y\nonumber \\
	H_1&: \beta_1 \ne 0
\end{align}
using the $t$-statistic
\begin{align}
	t = \frac{\hat{\beta}_1 - 0}{\mathrm{SE}\left(\hat{\beta}_1\right)},
\end{align}
which measures the number of standard deviations that $\hat{\beta}_1$ is away from the mean 0, because if there is no relationship between $X$ and $Y$, then $t$ will have a $t$-distribution with $df = n - 2$, that is, the degrees of freedom = the number of variables $-$ the number of parameters.

## Assessing the Accuracy of the Model

### Residual Standard Error

The *residual standard error* is
\begin{align}
	\mathrm{RSE} &= 
		\sqrt{\frac{1}{n-2}\mathrm{RSS}}\nonumber \\
		&=\sqrt{\frac{1}{n-2}\Sigma_{i=1}^n\left(y_i - \hat{y}_i\right)^2} \\
		&\approx  		\mathrm{Var}(\epsilon)^2 \nonumber
\end{align}
where the *residual sum of squares* is
\begin{align}
	\mathrm{RSS} =\Sigma_{i=1}^n\left(y_i - \hat{y}_i\right)^2. 
\end{align}

### $R^2$ Statistic

The formula for $R^2$ is
\begin{align}
	R^2 = \frac{\mathrm{TSS} - \mathrm{RSS}}{tss},
\end{align}
where the *total sum of squares* is

$$
\begin{align*}
	\mathrm{TSS} =\Sigma_{i=1}^n\left(y_i - \bar{y}\right)^2. 
\end{align*}
$$


The *correlation* of $X$ and $Y$ is
\begin{align}
r = \mathrm{Cor}(X, Y) = \frac{\Sigma_{i = 1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}
        {\sqrt{\Sigma_{i = 1}^n\left(x_i - \bar{x}\right)}
					\sqrt{\Sigma_{i = 1}^n\left(y_i - \bar{y}\right)}}
\end{align}
and in simple linear regression $$r^2 = R^2.$$

* Questions

