---
title: "ISLR Chap 3 Notes"
author: "M. E. Waggoner"
date: "February 3, 2019"
output: 
  pdf_document:
    toc: yes
    toc_depth: 5
    pandoc_args: [
      "--number-sections",
      "--number-offset=2"
      "--number-offset=3"
    ]
---

```{r setup, include=FALSE}
require(tidyverse)    # various
require(texreg)       # screenreg()
require(ggfortify)    # autoplot()
require(knitr)        # kable()
require(broom)        # tidy()

options(digits = 4)
```


$\setcounter{section}{2}$

# Linear Regression

## Simple Linear Regression

Note:  The section numbers of this document match up with ISLR.  The equation numbers match up with the section, but you need to append "3." to the number.

The statistics behind most machine learning assumes that the data we have is just one sample of a much larger population.  Thus, when we calculate means and standard deviations, we are calculating them from a sample, i.e., 
$$\displaystyle\bar{x} = \frac{1}{n}\Sigma_{i-1}^n x_i.$$
Sample statistics methods are what are behind the hypothesis testing and confidence intervals we will use.



$y$ is aproximately a linear function of $X$, but unknown
\begin{align}
Y \approx \beta_0 + \beta_1 X
\end{align}

Since we don't know the slope and intercept, we find approximations for them using the training data.
\begin{align}
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x
\end{align}

### Estimating the Coefficients

$i$th Residual.  Note that this is the difference between the actual value and the estimated linear relationship, which is not the same as $\epsilon_i$, which is the difference between the actual value and the true line.
\begin{align*}
e_i = y_i - \hat{y}_i 
\end{align*}

Residual sum of squares = what is minimized by least squares.  I  said that MSE was minimized by least squares, and it is since $MSE = RSS/n$. 
\begin{align}
	\mathrm{RSS} =& e_1^2	+ e_2^2 + \cdots + e_n^2 \nonumber \\ 
	=& \left( y_1 - \hat{y}_1\right)^2 + \left( y_2 - \hat{y}_2\right)^2 + \cdots + \left( y_n - \hat{y}_n\right)^2 \\
	=& \Sigma_{i-1}^n \left( y_i - \hat{y}_i\right)^2 \nonumber
\end{align}

Least squares coefficients estimates that minimize RSS where $\bar{x}$ and $\bar{y}$ as calculated by `lm()`. 
\begin{align}
\hat{\beta}_1 =& \frac{\Sigma_{i = 1}^{n} \left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}
                    {\Sigma_{i = 1}^{n} \left(x_i - \bar{x}\right)^2} \nonumber \\
\hat{\beta}_0	=& \bar{y} - \hat{\beta}_1 \bar{x},
\end{align}
where $\bar{x}$ and $\bar{y}$ are means of $x$ and $y$, respectively.





### Assessing the Accuracy of the Coefficient Estimates

These statistics assess the accuracy of the parameters of slope and intercept, and do not assess the accuracy of the model.  In other words, these statistics are evaluating our ability to use the estimates of the slope and intercept, but they do not tell us whether a linear model is a good fit to the data.

True linear relationship between $X$ and $Y$
\begin{align}
	Y = \beta_0 + \beta_1 X + \epsilon \label{3.}
\end{align}

$\stepcounter{equation}$

The standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution or an estimate of that standard deviation.

Standard error of a sample mean where $\sigma^2 = \mathrm{Var}(x)$ for a population $X$
\begin{align}
	\mathrm{Var}\left(\hat{\mu}\right) = \mathrm{SE}\left(\hat{\mu}\right)^2 = \frac{\sigma^2}{n}
\end{align}

In general, the standard deviation of a distribution $x$ is approximated by the standard error of $x$.
\begin{align*}
	\mathrm{Var}(x) = \sigma(x)\approx\mathrm{SE}\left(\hat x\right)^2 
\end{align*}

Standard error of estimates of linear coefficients where $\sigma^2 = \mathrm{Var}(\epsilon)$ are calculated by `lm()`.
\begin{align}
	\mathrm{SE}\left(\hat{\beta}_0\right)^2 & = 
		\sigma^2\left[ 
			\frac{1}
					 {n} + 
			\frac{\bar{x}^2}
					 {\Sigma_{i - 1}^n\left(x_i - \bar{x}^2\right)} \right] 
			\nonumber \\
	\mathrm{SE}\left(\hat{\beta}_1\right)^2 & = 
		 \frac{\sigma{x}^2}
					{\Sigma_{i - 1}^n \left(x_i - \bar{x}^2\right)}
\end{align}

$\stepcounter{equation}$

We will use the standard errors in two ways.  In practice, usually one or the other is used.

#### Confidence intervals

There is approximately a 95\% chance that the true value of the slope $\beta_1$ lies in the interval
\begin{align}
	\left[ 	\hat{\beta}_1 - 2 \mathrm{SE}\left( \hat{\beta}_1 \right),
					\hat{\beta}_1 + 2 \mathrm{SE}\left( \hat{\beta}_1 \right) 
					\right].
\end{align}
If the confidence interval for the slope contains 0, this is an indication that the slope very well might be 0 and that $Y$ does not depend linearly on $X$.

There is approximately a 95% chance that the true value of the intercept $\beta_0$ lies in the interval
\begin{align}
	\left[ 	\hat{\beta}_0 - 2 \mathrm{SE}\left( \hat{\beta}_0 \right),
					\hat{\beta}_0 + 2 \mathrm{SE}\left( \hat{\beta}_0 \right) 
					\right].
\end{align}
Generally, we will use a confidence level of 95%, but in practice the confidence level will be determined by the application, the client, and the industry.

##### Example: How large is the effect?

Consider the `mtcars` dataset that comes with base R.  Is the miles per gallon `disp` a car can get a linear relationship of `mpg`?  If so, how large is the effect?  That is, what is the change in `mpg` for an increase or decrease of 1 `disp`?

```{r echo = FALSE}
mtcars %>% 
  ggplot(aes(x = disp, y = mpg)) + geom_point() +
  geom_smooth(method = "lm", color = "red")
lmCar <- lm(mpg ~ disp , data = mtcars)
m <- coef(lmCar)[[2]]
b <- coef(lmCar)[[1]]
lowb <- confint(lmCar)[1, 1]
lowm <- confint(lmCar)[2, 1]
highb <- confint(lmCar)[1, 2]
highm <- confint(lmCar)[2, 2]
SEm <- tidy(lmCar)[2, 3][[1]]
SEb <- tidy(lmCar)[1, 3][[1]]
```

We can view the coefficients and their standard errors using the `tidy()` function.  

```{r echo = FALSE}
kable(tidy(lmCar))
```

The estimate of the slope is $\hat\beta_1 = `r m`$ with $\mathrm{SE}\left(\hat\beta_1\right) = `r SEm`$.  Thus, there is a 95% chance that the true slope is in the interval $[`r lowm`, `r highm`]$. This interval is relatively narrow since $\frac{\mathrm{SE}\left(\hat\beta_1\right)}{\hat\beta_1} = `r SEm/m`$ is small.  This interval is "far" from zero since it does not contain zero and the distance from the interval to zero relatively large since $\left|\frac{\hat\beta_1 + \mathrm{SE}\left(\hat\beta_1\right)}{\mathrm{SE}\left(\hat\beta_1\right)}\right| = `r abs((SEm + m)/SEm)`$ is large.

The estimate of the intercept is $\hat\beta_0 = `r b`$ with $\mathrm{SE}\left(\hat\beta_0\right) = `r SEb`$.  Thus, there is a 95% chance that the true intercept is in the interval $[`r lowb`, `r highb`]$. 

This information can be found with the `confint()` function.



```{r echo = FALSE}
kable(confint(lmCar))


```

So how large is the effect of `mpg` on `disp`? The change in `disp` given a change in `mpg` is $$\frac{dy}{dx} = \frac{d}{dx}\left( \beta_1 x + \beta_0 \right) = \beta_1.$$ Thus, there is a change of $-0.0417 \pm 0.0047$ miles per gallon for each change of 1 unit of displacement.

#### Hypothesis testing

A common hypothesis test of linear relationships tests the null hypothesis
\begin{align}
	H_0&: \text{There is no relationship between }X\text{ and }Y\nonumber \\
	H_0&: \beta_1 = 0
\end{align}
versus the alternate hypothesis
\begin{align}
	H_1&: \text{There is a relationship between }X\text{ and }Y\nonumber \\
	H_1&: \beta_1 \ne 0
\end{align}
using the $t$-statistic
\begin{align}
	t = \frac{\hat{\beta}_1 - 0}{\mathrm{SE}\left(\hat{\beta}_1\right)} 
\end{align}
which measures the number of standard deviations that $\hat{\beta}_1$ is away from the mean 0, because if there is no relationship between $X$ and $Y$, then $t$ will have a $t$-distribution with $df = n - 2$, that is, the degrees of freedom = the number of observations $-$ the number of parameters.

##### Example: Is there a relationship?

Continuing the example of `mtcars`, we can ask if there a relationship between miles per gallon and engine displacement.  For simple linear regression, this question can be answered with some level of confidence using a hypothesis test.  The null hypothesis is that there is no relationship, and it is assumed until we have evidence to the contrary.  The alternate hypothesis is what we will believe if there is statistical evidence.
\begin{align*}
	H_0&: \text{There is no relationship between mpg and displacement or }\beta_1 = 0 \\
	H_1&: \text{There is a relationship between mpg and displacement or }\beta_1 \ne 0
\end{align*}
We'll illustrate in another lab that the distribution of all possible slopes is approximately the $t$-distribution with $n - 2$ degrees of freedom.  The $t$ statistic for the slope for this model is
$$ t = \frac{\hat{\beta}_1 - 0}{\mathrm{SE}\left(\hat{\beta}_1\right)}  = \frac{`r m`}{`r SEm`} = `r m/SEm`. $$
At a significance level of 5%, we will reject the null hypothesis if $$p = P(|t| > |-8.747| ) < 5\%.$$  To put this in perspective, here is the plot of the $t$-distribution for $\mathrm{df} = 30$.  I think $-8.747$ is somewhere in the next room.  

```{r}
tdist <- tibble(x = seq(-4, 4, 0.1),
                y = dt(x, df = 30))
tdist %>% ggplot(aes(x, y)) + geom_path()
```

From the output of `lm()`, we see that $p = 0 < 5\%$.  Therefore, at the 5% significance level we can conclude that there is a relationship between miles per gallon and engine displacement.  At this point we don't know if we have modeled that relationship well or not.

We can ask the same thing about the `mpg`-intercept:  is there evidence that the intercept is not zero? The null hypothesis is that the intercept is zero, and it is assumed until we have evidence to the contrary.  The alternate hypothesis is what we will believe if there is statistical evidence.
\begin{align*}
	H_0&: \text{The intercept is zero or }\beta_0 = 0 \\
	H_1&: \text{The intercept is not zero or }\beta_0 \ne 0
\end{align*}
We can demonstrate that the distribution of all possible intercepts is approximately the $t$-distribution with $n - 2 = 30$ degrees of freedom for `mtcars`.  The $t$ statistic for the intercept for this model is
$$ t = \frac{\hat{\beta}_0 - 0}{\mathrm{SE}\left(\hat{\beta}_0\right)}  = \frac{`r b`}{`r SEb`} = `r b/SEb`. $$
At a significance level of 5%, we will reject the null hypothesis if the area beyond this $t$-statistic is less than 5%.  That is, we reject the null hypothesis if $$p = P(|t| > |24.07|) < 5\%.$$  The $t$-distribution for the intercept is the same as for the slope.   From the output of `lm()`, we see that $p = 0 < 5\%$ for the intercept.  Therefore, at the 5% significance level we can conclude that the `mpg`-intercept is not zero.  However, we do not know yet whether we have made a good estimate of the intercept or not.

### Assessing the Accuracy of the Model

These statistics allow us to assess the accuracy of the model; that is, they help us know whether a linear model was a good choice.

#### Residual Standard Error

A small RSE *relative to the data* indicates that the actual data are close to the predictions, and we can say the model fits the data well.  

A large RSE *relative to the data* indicates that the actual data are not close to the predictions, and we can say the model does not fit the data well.

The *residual standard error* is
\begin{align}
	\mathrm{RSE} &= 
		\sqrt{\frac{1}{n-2}\mathrm{RSS}}\nonumber \\
		&=\sqrt{\frac{1}{n-2}\Sigma_{i=1}^n\left(y_i - \hat{y}_i\right)^2} \\
		&\approx  		\mathrm{Var}(\epsilon)^2 \nonumber
\end{align}
where the *residual sum of squares* is
\begin{align}
	\mathrm{RSS} =\Sigma_{i=1}^n\left(y_i - \hat{y}_i\right)^2. 
\end{align}



#### $R^2$ Statistic

The formula for $R^2$ is
\begin{align}
	R^2 = \frac{\mathrm{TSS} - \mathrm{RSS}}{\mathrm{RSS}},
\end{align}
where the *total sum of squares* is


\begin{align*}
	\mathrm{TSS} =\Sigma_{i=1}^n\left(y_i - \bar{y}\right)^2. 
\end{align*}



The *correlation* of $X$ and $Y$ is
\begin{align}
r = \mathrm{Cor}(X, Y) = \frac{\Sigma_{i = 1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}
        {\sqrt{\Sigma_{i = 1}^n\left(x_i - \bar{x}\right)}
					\sqrt{\Sigma_{i = 1}^n\left(y_i - \bar{y}\right)}}
\end{align}
and in simple linear regression $$r^2 = R^2.$$

##### Examples:  RSE versus $R^2$

Or, Residual Squared Error vs Coefficient of Determination. 

Consider this simulated data.

```{r echo = FALSE, fig.height=5}
set.seed(111)
myData <- tibble(x = runif(n =100, 
                           min = 1, 
                           max = 100),
                 y1 = 20*x+ 50 + 200*rnorm(n = 100 ),
                 y2 = x^3/600 +20*x - 350 + 
                   30*sample(-1:1,100,replace=T) ,
                 y3 = 15*x*sample(0:1,100,replace=T) +10*x+ 50 + 
                   100*rnorm(n = 100),
                 y4 = 20*x + 50 + 400*rnorm(n = 100 ) )

myData %>% 
  gather(key = "dataset", value = "y", -x) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  stat_smooth(method = "lm", col = "red") +
  facet_wrap(facets = vars(dataset))

lm1 <- lm(y1 ~ x, data = myData)
lm2 <- lm(y2 ~ x, data = myData)
lm3 <- lm(y3 ~ x, data = myData)
lm4 <- lm(y4 ~ x, data = myData)

mean_y <- myData %>% 
  summarize(meany1 = mean(y1),
          meany2 = mean(y2),
          meany3 = mean(y3),
          meany4 = mean(y4))

mean_y

screenreg(list(lm1, lm2, lm3, lm4))



```

##### Example: How strong (i.e., accurate) is the relationship?

```{r echo = FALSE}
ybar <- mean(mtcars$disp)
rse <- summary(lmCar)$sigma
```


Let's return to the `mtcars` example, for which some statistics are listed below.

Using residual error, we see that $\mu\left(\mathrm{disp}\right) = `r ybar`$ while the $\mathrm{RSE}  = `r rse`$.  Thus, there is a relative error of roughly $\frac{\mathrm{RSE}}{\mu\left(\mathrm{disp}\right)} = `r rse/ybar*100`\%$.  This seems small, but whether it is small enough depends on the application and the context.

The *coefficient of determination* is $R^2 = 0.72$.  This says that 72% of the variation in miles per gallon is explained by this linear model and that 28% of the variation is unexplained by the model, and must either be considered error or the model needs to be changed to account for this variation.

```{r}
screenreg(list(lmCar))
```



### Looking to the future:  Section 3.3.3

As we can see from the previous section, the values of $R^2$ and RSE do not tell the whole story.  We would look at a variety of analyses to help us understand the structure of the data and find a good model.

One such analysis is to look at the residuals.  If our model accounts for all variation in $Y$ that is based on $X$,  then the residuals $e_i$ should be a good estimate for the errors $\epsilon_i$.  Thus, the residuals should be random with a normal distribution and have not other structure.  If they do not have these qualities, then there is more to the relationship of $X$ and $Y$ than we accounted for in the regression.

There are many ways to look the distribution of the residuals.  Here we look at their distribution as a scatterplot and a normal QQ plot.  These are created automatically when we call `lm()`.  The command `ggfortify::autoplot()` is one way to view the data, but there are many others.

The first model `lm1` shows exactly what the residuals should look like if the linear model captured all the variation in $Y$ that depends on $X$.  The QQ plot shows that the distribution is normal.  The scatterplot of the residuals shows that about 2/3 of the points are within one standard deviation of the mean, and almost all are within 2 standard deviations.  The blue fitted line should be the line $e = 0$ if the residuals are centered around 0, and we see here that it is close.



```{r}
autoplot(lm1, which = c(2, 1), ncol = 2, label.size = 3)
```

The second model `lm2` shows poor results in the residual analysis, and there is clearly a relationship between $X$ and $Y$ that was not capture by the linear model. The QQ plot shows that the residuals are not normally distributed.  The scatterplot of the residuals has a clear U-shaped pattern, and the residuals are not random.  In this case, it would be good to try a quadratic rather than linear model.



```{r}
autoplot(lm2, which = c(2, 1), ncol = 2, label.size = 3)
```

The third model `lm3` also shows there is a relationship between $X$ and $Y$ that was not capture by the linear model. This time the scatterplot of the residuals has a sideways funnel shape, but the blue fitted line still hugs the horizontal axis.  In this case, it would be good to a Box-Cox transformation, possibly a log transformation.  



```{r}
autoplot(lm3, which = c(2, 1), ncol = 2, label.size = 3)
```

The fourth model `lm4` is similar to the first model:  the residuals are random and normally distributed.



```{r}
autoplot(lm4, which = c(2, 1), ncol = 2, label.size = 3)
```
